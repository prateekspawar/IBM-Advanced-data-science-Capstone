{"cells": [{"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "Here we evaluate the performance of different models generated."}, {"metadata": {}, "cell_type": "markdown", "source": "Lets import the test data first"}, {"metadata": {}, "cell_type": "code", "source": "import types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share the notebook.\ncos = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='ng6mKmGtGPTwjl1w-iDYC0hvQW0CDv_bM2qXoyl3oZRL',\n    ibm_auth_endpoint=\"https://iam.eu-gb.bluemix.net/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3.eu-geo.objectstorage.service.networklayer.com')\n\nbody = cos.get_object(Bucket='foodimpactcovid-donotdelete-pr-ckuilhhenkqwsg',Key='df_test.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\ndf_test = pd.read_csv(body)\n", "execution_count": 3, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "\nimport types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share the notebook.\nclient_8e01ca4d30994b239a53c23104c881fd = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='ng6mKmGtGPTwjl1w-iDYC0hvQW0CDv_bM2qXoyl3oZRL',\n    ibm_auth_endpoint=\"https://iam.eu-gb.bluemix.net/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3.eu-geo.objectstorage.service.networklayer.com')\n\nbody = client_8e01ca4d30994b239a53c23104c881fd.get_object(Bucket='foodimpactcovid-donotdelete-pr-ckuilhhenkqwsg',Key='df_train.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\ndf_train = pd.read_csv(body)\n\n", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import numpy as np\nfrom keras.models import model_from_json\nfrom keras.models import load_model\nimport json\nX_train = df_train[df_train.columns[1:17]] \nY_train = df_train['Confirmed']\nX_test = df_test[df_test.columns[1:17]] \nY_test = df_test['Confirmed']\nX_train_np=X_train.to_numpy()\nY_train_np=Y_train.to_numpy()\nX_test_np=X_test.to_numpy()\nY_test_np=Y_test.to_numpy()", "execution_count": 5, "outputs": [{"output_type": "stream", "text": "Using TensorFlow backend.\n", "name": "stderr"}]}, {"metadata": {}, "cell_type": "code", "source": "credentials_1 = {\n    'IAM_SERVICE_ID': 'iam-ServiceId-e8071002-7c9e-408a-9f8f-7095f09bcb51',\n    'IBM_API_KEY_ID': 'ng6mKmGtGPTwjl1w-iDYC0hvQW0CDv_bM2qXoyl3oZRL',\n    'ENDPOINT': 'https://s3.eu-geo.objectstorage.service.networklayer.com',\n    'IBM_AUTH_ENDPOINT': 'https://iam.eu-gb.bluemix.net/oidc/token',\n    'BUCKET': 'foodimpactcovid-donotdelete-pr-ckuilhhenkqwsg',\n    'FILE': 'Food_Supply_kcal_Data.csv'\n}", "execution_count": 6, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Download models from Cloud object store."}, {"metadata": {}, "cell_type": "code", "source": "cos.download_file(Bucket=credentials_1['BUCKET'],Key='model1.pickle',Filename='model1.pickle')\ncos.download_file(Bucket=credentials_1['BUCKET'],Key='model2.pickle',Filename='model2.pickle')\ncos.download_file(Bucket=credentials_1['BUCKET'],Key='model3.h5',Filename='model3.h5')\ncos.download_file(Bucket=credentials_1['BUCKET'],Key='model4.h5',Filename='model4.h5')", "execution_count": 7, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "First consider the deep learning model without regularization."}, {"metadata": {}, "cell_type": "code", "source": "model3=load_model(\"model3.h5\")", "execution_count": 8, "outputs": [{"output_type": "stream", "text": "WARNING:tensorflow:From /opt/conda/envs/Python36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From /opt/conda/envs/Python36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.metrics import mean_squared_error\ny3_pred=model3.predict(X_test_np)\nfor i in range(len(y3_pred)):\n    if(y3_pred[i]<0):\n        y3_pred[i]=0\nmodel_3_mse=mean_squared_error(Y_test_np, y3_pred)\nmodel_3_mse", "execution_count": 9, "outputs": [{"output_type": "execute_result", "execution_count": 9, "data": {"text/plain": "0.004365087735385307"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Now consider the deep learning model with regularization."}, {"metadata": {}, "cell_type": "code", "source": "model4=load_model(\"model4.h5\")", "execution_count": 10, "outputs": [{"output_type": "stream", "text": "WARNING:tensorflow:From /opt/conda/envs/Python36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "y4_pred=model4.predict(X_test_np)\nfor i in range(len(y4_pred)):\n    if(y4_pred[i]<0):\n        y4_pred[i]=0\nmodel_4_mse=mean_squared_error(Y_test_np, y4_pred)\nmodel_4_mse", "execution_count": 11, "outputs": [{"output_type": "execute_result", "execution_count": 11, "data": {"text/plain": "0.0039004348660493216"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "import pickle", "execution_count": 12, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Consider the SVR model with all the features."}, {"metadata": {}, "cell_type": "code", "source": "model1 = pickle.load(open('model1.pickle', 'rb'))", "execution_count": 13, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "y1_pred=model1.predict(X_test_np)", "execution_count": 14, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "for i in range(len(y1_pred)):\n    if(y1_pred[i]<0):\n        y1_pred[i]=0\n", "execution_count": 15, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "model_1_mse=mean_squared_error(Y_test_np, y1_pred)\nmodel_1_mse", "execution_count": 16, "outputs": [{"output_type": "execute_result", "execution_count": 16, "data": {"text/plain": "0.001602298513105709"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Consider the SVR model in which feature reduction with PCA is done."}, {"metadata": {}, "cell_type": "code", "source": "model2 = pickle.load(open('model2.pickle', 'rb'))", "execution_count": 18, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.decomposition import PCA\npca = PCA(n_components=5)\nX_pca=pca.fit_transform(X_train_np)\nX_test_pca=pca.transform(X_test_np)", "execution_count": 19, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "y2_pred=model2.predict(X_test_pca)\nfor i in range(len(y2_pred)):\n    if(y2_pred[i]<0):\n        y2_pred[i]=0\n", "execution_count": 20, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "model_2_mse=mean_squared_error(Y_test_np, y2_pred)\nmodel_2_mse", "execution_count": 21, "outputs": [{"output_type": "execute_result", "execution_count": 21, "data": {"text/plain": "0.0010448858154379652"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "From the above evaluation, we can conclude that SVR model using feature reduction gives best result in the 4 models considered here."}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}